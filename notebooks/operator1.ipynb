{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operator 1 -- Company Analysis Pipeline\n",
    "\n",
    "**Guarantees:**\n",
    "- No look-ahead bias: all statement data aligned via as-of logic\n",
    "- Ratio safety: all divisions guarded against zero/null/tiny denominators\n",
    "- Missing data tracked: every feature has an `is_missing_*` companion flag\n",
    "- Observed values never overwritten by estimation\n",
    "- Secrets never printed or logged\n",
    "- Idempotent cells: cached data reused unless `FORCE_REBUILD=True`\n",
    "\n",
    "**Pipeline Phases:**\n",
    "1. Foundation (secrets, config, HTTP, clients)\n",
    "2. Ingestion (verification, macro, entity discovery, extraction)\n",
    "3. Cache & Features (daily cache, derived variables, linked aggregates)\n",
    "4. Analysis (survival mode, vanity, hierarchy weights, data quality)\n",
    "5. Estimation (deterministic identity fill + regime-weighted imputation)\n",
    "6. Temporal Modeling (regimes, forecasting, Monte Carlo, predictions)\n",
    "7. Report (profile JSON, Gemini narrative, charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Inputs\n",
    "# ============================================================\n",
    "# User-provided identifiers -- the ONLY required manual inputs.\n",
    "# Do NOT hardcode company name or country; those are extracted\n",
    "# automatically from the Eulerpool profile.\n",
    "\n",
    "target_isin = \"US0378331005\"   # Example: Apple Inc.\n",
    "fmp_symbol  = \"AAPL\"           # FMP ticker for OHLCV\n",
    "\n",
    "print(f\"Target ISIN : {target_isin}\")\n",
    "print(f\"FMP Symbol  : {fmp_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load secrets\n",
    "# ============================================================\n",
    "from operator1.secrets_loader import get_secret\n",
    "\n",
    "EULERPOOL_API_KEY = get_secret(\"EULERPOOL_API_KEY\")\n",
    "EOD_API_KEY       = get_secret(\"EOD_API_KEY\")\n",
    "FMP_API_KEY       = get_secret(\"FMP_API_KEY\")\n",
    "GEMINI_API_KEY    = get_secret(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Verify keys loaded (never print values!)\n",
    "for name, key in [(\"EULERPOOL\", EULERPOOL_API_KEY),\n",
    "                   (\"EOD\", EOD_API_KEY),\n",
    "                   (\"FMP\", FMP_API_KEY),\n",
    "                   (\"GEMINI\", GEMINI_API_KEY)]:\n",
    "    status = \"OK\" if key else \"MISSING\"\n",
    "    print(f\"{name}_API_KEY: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Global config\n",
    "# ============================================================\n",
    "from operator1.config_loader import get_global_config, load_config\n",
    "from operator1.constants import DATE_START, DATE_END, CACHE_DIR\n",
    "import os\n",
    "\n",
    "cfg = get_global_config()\n",
    "FORCE_REBUILD = cfg.get(\"FORCE_REBUILD\", False)\n",
    "\n",
    "print(f\"Date range  : {DATE_START} to {DATE_END}\")\n",
    "print(f\"Timeout     : {cfg.get('timeout_s', 30)}s\")\n",
    "print(f\"Max retries : {cfg.get('max_retries', 3)}\")\n",
    "print(f\"FORCE_REBUILD: {FORCE_REBUILD}\")\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: HTTP utilities\n",
    "# ============================================================\n",
    "from operator1.http_utils import cached_get, get_request_log\n",
    "\n",
    "print(\"HTTP utilities loaded.\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Eulerpool client\n",
    "# ============================================================\n",
    "from operator1.clients.equity_provider import create_equity_provider\n",
    "from operator1.secrets_loader import load_secrets\n",
    "\n",
    "secrets = load_secrets()\n",
    "equity_client = create_equity_provider(secrets)\n",
    "print(\"Equity data client initialised.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: FMP client\n",
    "# ============================================================\n",
    "from operator1.clients.fmp import FMPClient\n",
    "\n",
    "fmp = FMPClient(api_key=FMP_API_KEY)\n",
    "print(\"FMP client initialised.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Identifier verification + country extraction\n",
    "# ============================================================\n",
    "# Fail fast on invalid ISIN or FMP symbol.\n",
    "from operator1.steps.verify_identifiers import verify_identifiers\n",
    "\n",
    "verified = verify_identifiers(\n",
    "    target_isin=target_isin,\n",
    "    fmp_symbol=fmp_symbol,\n",
    "    eulerpool_client=equity_client,\n",
    "    fmp_client=fmp,\n",
    ")\n",
    "\n",
    "print(f\"Verified: {verified.name} ({verified.ticker})\")\n",
    "print(f\"Country : {verified.country}\")\n",
    "print(f\"Sector  : {verified.sector}\")\n",
    "print(f\"Exchange: {verified.exchange}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: World Bank macro module\n",
    "# ============================================================\n",
    "from operator1.steps.macro_mapping import fetch_macro_data\n",
    "from operator1.clients.world_bank import WorldBankClient\n",
    "\n",
    "wb_client = WorldBankClient()\n",
    "macro_data = fetch_macro_data(\n",
    "    country_iso2=verified.country,\n",
    "    wb_client=wb_client,\n",
    "    force_rebuild=FORCE_REBUILD,\n",
    ")\n",
    "\n",
    "print(f\"Macro variables fetched: {len(macro_data)} indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Linked entity discovery (Gemini)\n",
    "# ============================================================\n",
    "from operator1.clients.gemini import GeminiClient\n",
    "from operator1.steps.entity_discovery import discover_linked_entities\n",
    "\n",
    "gemini = GeminiClient(api_key=GEMINI_API_KEY)\n",
    "\n",
    "try:\n",
    "    linked_entities = discover_linked_entities(\n",
    "        verified_target=verified,\n",
    "        gemini_client=gemini,\n",
    "        eulerpool_client=equity_client,\n",
    "        force_rebuild=FORCE_REBUILD,\n",
    "    )\n",
    "    print(f\"Linked entities discovered: {sum(len(v) for v in linked_entities.values())}\")\n",
    "    for group, entities in linked_entities.items():\n",
    "        print(f\"  {group}: {len(entities)} entities\")\n",
    "except Exception as exc:\n",
    "    print(f\"Entity discovery failed (non-fatal): {exc}\")\n",
    "    linked_entities = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Linked entity resolution + checkpointing\n",
    "# ============================================================\n",
    "# Progress is checkpointed to cache/progress.json.\n",
    "# On rerun, already-resolved entities are loaded from cache.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "progress_path = Path(CACHE_DIR) / \"progress.json\"\n",
    "if progress_path.exists() and not FORCE_REBUILD:\n",
    "    with open(progress_path, \"r\") as f:\n",
    "        progress = json.load(f)\n",
    "    print(f\"Loaded progress checkpoint: {len(progress.get('resolved', {}))} entities\")\n",
    "else:\n",
    "    progress = {\"resolved\": {}, \"failed\": []}\n",
    "    print(\"Starting fresh entity resolution.\")\n",
    "\n",
    "# Save progress\n",
    "with open(progress_path, \"w\") as f:\n",
    "    json.dump(progress, f, indent=2)\n",
    "print(\"Progress checkpointed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Target OHLCV build (FMP authoritative)\n",
    "# ============================================================\n",
    "from operator1.steps.data_extraction import extract_target_ohlcv\n",
    "\n",
    "target_ohlcv = extract_target_ohlcv(\n",
    "    fmp_client=fmp,\n",
    "    fmp_symbol=fmp_symbol,\n",
    "    date_start=DATE_START,\n",
    "    date_end=DATE_END,\n",
    "    force_rebuild=FORCE_REBUILD,\n",
    ")\n",
    "\n",
    "print(f\"Target OHLCV: {len(target_ohlcv)} rows\")\n",
    "print(f\"Date range  : {target_ohlcv.index.min()} to {target_ohlcv.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Target fundamentals as-of table\n",
    "# ============================================================\n",
    "from operator1.steps.cache_builder import build_target_cache\n",
    "\n",
    "target_cache = build_target_cache(\n",
    "    verified_target=verified,\n",
    "    ohlcv=target_ohlcv,\n",
    "    eulerpool_client=equity_client,\n",
    "    date_start=DATE_START,\n",
    "    date_end=DATE_END,\n",
    "    force_rebuild=FORCE_REBUILD,\n",
    ")\n",
    "\n",
    "print(f\"Target cache: {target_cache.shape[0]} days x {target_cache.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Feature engineering + safety flags\n",
    "# ============================================================\n",
    "from operator1.features.derived_variables import compute_derived_variables\n",
    "from operator1.features.macro_alignment import align_macro_to_daily\n",
    "from operator1.quality.data_quality import run_quality_audit\n",
    "\n",
    "# Compute derived variables (returns, solvency, liquidity, etc.)\n",
    "target_cache = compute_derived_variables(target_cache)\n",
    "print(f\"After derived vars: {target_cache.shape[1]} columns\")\n",
    "\n",
    "# Align macro data to daily\n",
    "try:\n",
    "    target_cache = align_macro_to_daily(target_cache, macro_data)\n",
    "    print(f\"After macro alignment: {target_cache.shape[1]} columns\")\n",
    "except Exception as exc:\n",
    "    print(f\"Macro alignment failed (non-fatal): {exc}\")\n",
    "\n",
    "# Data quality audit (fails on look-ahead violation)\n",
    "quality_report = run_quality_audit(target_cache)\n",
    "print(f\"Quality audit: {quality_report.get('status', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Linked caches in batches\n",
    "# ============================================================\n",
    "from operator1.steps.cache_builder import build_linked_caches\n",
    "\n",
    "try:\n",
    "    linked_cache = build_linked_caches(\n",
    "        linked_entities=linked_entities,\n",
    "        eulerpool_client=equity_client,\n",
    "        date_start=DATE_START,\n",
    "        date_end=DATE_END,\n",
    "        force_rebuild=FORCE_REBUILD,\n",
    "    )\n",
    "    print(f\"Linked cache: {linked_cache.shape[0]} rows x {linked_cache.shape[1]} columns\")\n",
    "except Exception as exc:\n",
    "    print(f\"Linked cache build failed (non-fatal): {exc}\")\n",
    "    import pandas as pd\n",
    "    linked_cache = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Linked aggregates join\n",
    "# ============================================================\n",
    "from operator1.features.linked_aggregates import compute_linked_aggregates\n",
    "\n",
    "try:\n",
    "    linked_aggregates = compute_linked_aggregates(\n",
    "        target_cache=target_cache,\n",
    "        linked_cache=linked_cache,\n",
    "        linked_entities=linked_entities,\n",
    "    )\n",
    "    print(f\"Linked aggregates: {linked_aggregates.shape[1]} columns\")\n",
    "except Exception as exc:\n",
    "    print(f\"Linked aggregates failed (non-fatal): {exc}\")\n",
    "    import pandas as pd\n",
    "    linked_aggregates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Survival mode + hierarchy weights + vanity\n",
    "# ============================================================\n",
    "from operator1.analysis.survival_mode import compute_survival_flags\n",
    "from operator1.analysis.vanity import compute_vanity_percentage\n",
    "from operator1.analysis.hierarchy_weights import compute_hierarchy_weights\n",
    "\n",
    "# Survival flags\n",
    "target_cache = compute_survival_flags(\n",
    "    target_cache,\n",
    "    country=verified.country,\n",
    "    sector=verified.sector,\n",
    ")\n",
    "print(\"Survival flags computed.\")\n",
    "\n",
    "# Vanity percentage\n",
    "target_cache = compute_vanity_percentage(target_cache)\n",
    "print(f\"Vanity % (latest): {target_cache['vanity_percentage'].iloc[-1]:.1f}%\")\n",
    "\n",
    "# Hierarchy weights\n",
    "target_cache = compute_hierarchy_weights(target_cache)\n",
    "print(f\"Survival regime (latest): {target_cache['survival_regime'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Save cache artifacts\n",
    "# ============================================================\n",
    "import json\n",
    "from pathlib import Path\n",
    "from operator1.http_utils import get_request_log\n",
    "\n",
    "cache_dir = Path(CACHE_DIR)\n",
    "\n",
    "# Save caches as Parquet\n",
    "target_cache.to_parquet(cache_dir / \"target_company_daily.parquet\")\n",
    "print(\"Saved: target_company_daily.parquet\")\n",
    "\n",
    "if not linked_cache.empty:\n",
    "    linked_cache.to_parquet(cache_dir / \"linked_entities_daily.parquet\")\n",
    "    print(\"Saved: linked_entities_daily.parquet\")\n",
    "\n",
    "if not linked_aggregates.empty:\n",
    "    linked_aggregates.to_parquet(cache_dir / \"linked_aggregates_daily.parquet\")\n",
    "    print(\"Saved: linked_aggregates_daily.parquet\")\n",
    "\n",
    "target_cache.to_parquet(cache_dir / \"full_feature_table.parquet\")\n",
    "print(\"Saved: full_feature_table.parquet\")\n",
    "\n",
    "# Metadata\n",
    "from dataclasses import asdict\n",
    "metadata = {\n",
    "    \"target_isin\": target_isin,\n",
    "    \"fmp_symbol\": fmp_symbol,\n",
    "    \"verified_target\": {k: v for k, v in asdict(verified).items() if k != \"raw_profile\"},\n",
    "    \"date_start\": str(DATE_START),\n",
    "    \"date_end\": str(DATE_END),\n",
    "    \"n_linked_entities\": sum(len(v) for v in linked_entities.values()),\n",
    "    \"request_log_count\": len(get_request_log()),\n",
    "}\n",
    "with open(cache_dir / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(\"Saved: metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Modeling / prediction cells\n",
    "# ============================================================\n",
    "# Each model wrapped in try/except -- failures are non-fatal.\n",
    "\n",
    "from operator1.estimation.estimator import run_estimation\n",
    "from operator1.models.regime_detector import detect_regimes_and_breaks\n",
    "from operator1.models.forecasting import run_forecasting\n",
    "from operator1.models.monte_carlo import run_monte_carlo\n",
    "from operator1.models.prediction_aggregator import run_prediction_aggregation\n",
    "\n",
    "# --- Estimation ---\n",
    "try:\n",
    "    target_cache = run_estimation(target_cache)\n",
    "    print(\"Estimation: OK\")\n",
    "except Exception as exc:\n",
    "    print(f\"Estimation failed (non-fatal): {exc}\")\n",
    "\n",
    "# --- Regime detection ---\n",
    "regime_result = None\n",
    "try:\n",
    "    regime_result = detect_regimes_and_breaks(target_cache)\n",
    "    print(f\"Regime detection: OK (current regime: {target_cache.get('regime_label', ['N/A']).iloc[-1] if 'regime_label' in target_cache.columns else 'N/A'})\")\n",
    "except Exception as exc:\n",
    "    print(f\"Regime detection failed (non-fatal): {exc}\")\n",
    "\n",
    "# --- Forecasting ---\n",
    "forecast_result = None\n",
    "try:\n",
    "    forecast_result = run_forecasting(target_cache)\n",
    "    print(f\"Forecasting: OK ({len(forecast_result.forecasts)} variables)\")\n",
    "except Exception as exc:\n",
    "    print(f\"Forecasting failed (non-fatal): {exc}\")\n",
    "\n",
    "# --- Monte Carlo ---\n",
    "mc_result = None\n",
    "try:\n",
    "    mc_result = run_monte_carlo(target_cache)\n",
    "    print(f\"Monte Carlo: OK (survival prob = {mc_result.survival_probability_mean:.2%})\")\n",
    "except Exception as exc:\n",
    "    print(f\"Monte Carlo failed (non-fatal): {exc}\")\n",
    "\n",
    "# --- Prediction aggregation ---\n",
    "prediction_result = None\n",
    "try:\n",
    "    prediction_result = run_prediction_aggregation(\n",
    "        cache=target_cache,\n",
    "        forecast_result=forecast_result,\n",
    "        mc_result=mc_result,\n",
    "    )\n",
    "    print(f\"Prediction aggregation: OK ({prediction_result.n_models_available} models)\")\n",
    "except Exception as exc:\n",
    "    print(f\"Prediction aggregation failed (non-fatal): {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Report generation + LIMITATIONS\n",
    "# ============================================================\n",
    "from dataclasses import asdict\n",
    "from operator1.report.profile_builder import build_company_profile\n",
    "from operator1.report.report_generator import generate_report\n",
    "\n",
    "# Build company profile JSON\n",
    "profile = build_company_profile(\n",
    "    verified_target=asdict(verified),\n",
    "    cache=target_cache,\n",
    "    linked_aggregates=linked_aggregates if not linked_aggregates.empty else None,\n",
    "    regime_result=asdict(regime_result) if regime_result else None,\n",
    "    forecast_result=asdict(forecast_result) if forecast_result else None,\n",
    "    mc_result=asdict(mc_result) if mc_result else None,\n",
    "    prediction_result=asdict(prediction_result) if prediction_result else None,\n",
    ")\n",
    "print(f\"Profile built: {len(profile)} sections\")\n",
    "\n",
    "# Generate report (with Gemini if available, fallback otherwise)\n",
    "report_output = generate_report(\n",
    "    profile=profile,\n",
    "    gemini_client=gemini,\n",
    "    cache=target_cache,\n",
    "    generate_pdf=False,\n",
    "    generate_chart_images=True,\n",
    ")\n",
    "\n",
    "print(f\"Report saved to: {report_output['markdown_path']}\")\n",
    "print(f\"Charts generated: {len(report_output['chart_paths'])}\")\n",
    "\n",
    "# Display the report\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(report_output['markdown']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}